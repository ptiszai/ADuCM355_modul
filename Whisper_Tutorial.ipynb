{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ptiszai/ADuCM355_modul/blob/main/Whisper_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcCupwj3H0zk"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/keatonkraiger/Whisper-Transcribe-and-Translate-Tutorial/blob/main/Whisper_Tutorial.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "<a target=\"_blank\" href=\"https://youtu.be/i4Sgg-ptRzs\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ef/Youtube_logo.png\" alt=\"Tutorial Video\" width=\"24\" height=\"24\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sd2oxgWH0zm"
      },
      "source": [
        "- Click the above button to open this notebook in Google Colab.\n",
        "- We recommend then making a copy of the notebook to your Google Drive so you can save your work (File -> Save a copy in Drive).\n",
        "- You may also view the accompanying supplamentary tutorial video [here](https://youtu.be/i4Sgg-ptRzs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elZFueXpoBeX"
      },
      "source": [
        "# Using OpenAI's Whisper for Transcription, Translation, and Creating Caption Files\n",
        "OpenAI's Whisper is a general-purpose speech recognition model described in their 2022 [paper](https://arxiv.org/abs/2212.04356). This notebook is a practical introduction on how to use Whisper in Google Colab.\n",
        "\n",
        "Before diving into Whisper, it's important to set up your environment correctly. This guide will walk you through the process, ensuring that even if you're not technically inclined, you'll be able to use Whisper effectively. Note that you don't need to run Whisper in Colab, we are using it here for convenience and the ability to run the model on a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6sNXKTPrwOH"
      },
      "source": [
        "## Setting Up Google Colab\n",
        "Google Colab provides a convenient platform to run Python code in the cloud, with access to powerful computing resources, including GPUs. To ensure your Colab notebook runs smoothly, it's recommended to enable GPU acceleration which will speed up your transcription. Note however that a GPU is not strictly necessary to use Whisper. Here's how you can enable it on Colab:\n",
        "\n",
        "\n",
        "1.   Click on 'Runtime' in the top menu.\n",
        "2.   Select 'Change runtime type'.\n",
        "3.   In the dialog that appears, under 'Hardware accelerator', choose 'GPU' (the type doesn't matter so much right now)\n",
        "4.   Click 'Save'.\n",
        "\n",
        "By enabling the GPU, your notebook will run more efficiently, especially when dealing with large models like Whisper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPIwL68B5EsB"
      },
      "source": [
        "We can also see the size of our GPU's VRAM with the `!nvidia-smi` command in order to see how large of a model we can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5DQxLKla4udR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d33f2af-b3ce-4566-f8c5-6f0381fdbc85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov 30 09:51:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CooNwKbm5GE-"
      },
      "source": [
        "### Important Note\n",
        "\n",
        "In the following cells, you will often see an `!` symbol before the text/commands. This is because Colab cells expect *code*.\n",
        "\n",
        "By using `!`, we are telling Colab we typing a command instead of a piece of code! If you did not include a `!`, Colab would assume you are running (Python) code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yMQgCyH25erR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2322269-9fb5-4faa-caea-41d5858de7dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world!\n"
          ]
        }
      ],
      "source": [
        "print('Hello world!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf_ZMeiPsN6t"
      },
      "source": [
        "## Getting Started with Whisper\n",
        "\n",
        "Whisper is available through OpenAI's GitHub repository. To use Whisper, you need to install it along with its dependencies. This guide will take you through the process step-by-step, ensuring a smooth setup. You may follow along in parallel looking at the Github as well! First, to install Whisper:\n",
        "\n",
        "\n",
        "\n",
        "1.   **Install Whisper**: Run the command !pip install -U openai-whisper in a Colab cell to install the latest release of Whisper.\n",
        "2.   **Install FFMPEG**: Whisper requires FFMPEG for audio processing. Use the command !apt install ffmpeg in Colab to install it.\n",
        "3.   **Additional Dependencies:** In some cases, you might need additional dependencies like setuptools-rust. If you encounter any installation errors, run !pip install setuptools-rust.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LNWPxmUatKR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f281df-f62a-4cf3-8e3e-0cddaaab3981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-36lkzarn\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-36lkzarn\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=3e5e0858e3fd65abd189f2268cc558cb328003117d212b0a8e05f266a0348096\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4v45ysif/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-5kj_v4ea\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-5kj_v4ea\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=3f1ea475cb015c9f567a329ce62bfba04182511834889488631cc9ddaa03d658\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mqch8qt6/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "  Attempting uninstall: openai-whisper\n",
            "    Found existing installation: openai-whisper 20250625\n",
            "    Uninstalling openai-whisper-20250625:\n",
            "      Successfully uninstalled openai-whisper-20250625\n",
            "Successfully installed openai-whisper-20250625\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Collecting setuptools-rust\n",
            "  Downloading setuptools_rust-1.12.0-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: setuptools>=62.4 in /usr/local/lib/python3.12/dist-packages (from setuptools-rust) (75.2.0)\n",
            "Requirement already satisfied: semantic_version<3,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from setuptools-rust) (2.10.0)\n",
            "Downloading setuptools_rust-1.12.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: setuptools-rust\n",
            "Successfully installed setuptools-rust-1.12.0\n"
          ]
        }
      ],
      "source": [
        "#!pip install -U openai-whisper\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git\n",
        "!apt install ffmpeg\n",
        "!pip install setuptools-rust"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1nZ3oYq-SM3i",
        "outputId": "965d5144-3760-4a5f-8f58-eb0873f55ca2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update && sudo apt install ffmpeg"
      ],
      "metadata": {
        "id": "EbB7xXk5Telt",
        "outputId": "02459db6-89f0-4c0c-81da-043f30822e4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [1 InRelease 14.2 kB/129 kB 11%] [Connect\u001b[0m\r                                                                               \rGet:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [1 InRelease 43.1 kB/129\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [1 InRelease 129 kB/129 \u001b[0m\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://cli.github.com/packages stable/main amd64 Packages [343 B]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,286 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,535 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [60.9 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]\n",
            "Get:17 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.6 kB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,153 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,491 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,836 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,870 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,592 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Fetched 37.7 MB in 6s (6,092 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "94 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 94 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install ffmpeg"
      ],
      "metadata": {
        "id": "JwGXNBTaT2y8",
        "outputId": "41abcc89-d418-4ac3-a79e-fe80944e2140",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 94 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s61zPEmsq68"
      },
      "source": [
        "## Available Models\n",
        "\n",
        "Whisper comes with several models, each offering a trade-off between speed and accuracy. Depending on your task, you can choose the model that best fits your needs. Importantly, the *size* of the model (Required VRAM) will be dictated by the GPU running the code **if** you are using a GPU.\n",
        "\n",
        "In this case, we will use the *medium* sized model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSRbuM-_tzMG"
      },
      "source": [
        "## Get your audio files ready!\n",
        "\n",
        "Of course, our goal is to transcripe/translation an audio file. Thus, we will need to upload our files to colab. This can be achieved through\n",
        "\n",
        "1. Clicking the folder icon on the left sidebar at the bottom.\n",
        "2. Clicking the upload file button or dragging and dropping your file into the left side bar.\n",
        "3. For this demonstration, upload the `AllStar.mp3` file and `Cupid_Fifty_Fifty_Korean_Version.mp3` file from the GitHub [repository](https://github.com/keatonkraiger/Whisper-Transcribe-and-Translate-Tutorial).\n",
        "\n",
        "**Be aware**: <ins>Colab sessions *will not* save files that have been uploaded or created. Be sure to save files that are created during sessions before exiting them.</ins>\n",
        "\n",
        "**File Upload**: in the case you are uploading large files, you may instead want to mount your Google Drive to colab. You can then move files from Drive into Colab, which is often faster than uploading them individually. This can be achieved by clicking the Mount Drive button at the top left sidebar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9_Vy_EDtdEU"
      },
      "source": [
        "## Using Whisper\n",
        "\n",
        "Once again, following the Github documentation, we can run Whisper through command-line arguments.\n",
        "\n",
        "Let's first test whisper through the `whisper --help` command-line argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "REBmx_ALvcM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf9c850c-3957-4525-bd78-8d75b3b0a54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: whisper [-h] [--model MODEL] [--model_dir MODEL_DIR] [--device DEVICE]\n",
            "               [--output_dir OUTPUT_DIR]\n",
            "               [--output_format {txt,vtt,srt,tsv,json,all}]\n",
            "               [--verbose VERBOSE] [--task {transcribe,translate}]\n",
            "               [--language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}]\n",
            "               [--temperature TEMPERATURE] [--best_of BEST_OF]\n",
            "               [--beam_size BEAM_SIZE] [--patience PATIENCE]\n",
            "               [--length_penalty LENGTH_PENALTY]\n",
            "               [--suppress_tokens SUPPRESS_TOKENS]\n",
            "               [--initial_prompt INITIAL_PROMPT]\n",
            "               [--carry_initial_prompt CARRY_INITIAL_PROMPT]\n",
            "               [--condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT]\n",
            "               [--fp16 FP16]\n",
            "               [--temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK]\n",
            "               [--compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD]\n",
            "               [--logprob_threshold LOGPROB_THRESHOLD]\n",
            "               [--no_speech_threshold NO_SPEECH_THRESHOLD]\n",
            "               [--word_timestamps WORD_TIMESTAMPS]\n",
            "               [--prepend_punctuations PREPEND_PUNCTUATIONS]\n",
            "               [--append_punctuations APPEND_PUNCTUATIONS]\n",
            "               [--highlight_words HIGHLIGHT_WORDS]\n",
            "               [--max_line_width MAX_LINE_WIDTH]\n",
            "               [--max_line_count MAX_LINE_COUNT]\n",
            "               [--max_words_per_line MAX_WORDS_PER_LINE] [--threads THREADS]\n",
            "               [--clip_timestamps CLIP_TIMESTAMPS]\n",
            "               [--hallucination_silence_threshold HALLUCINATION_SILENCE_THRESHOLD]\n",
            "               audio [audio ...]\n",
            "\n",
            "positional arguments:\n",
            "  audio                 audio file(s) to transcribe\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model MODEL         name of the Whisper model to use (default: turbo)\n",
            "  --model_dir MODEL_DIR\n",
            "                        the path to save model files; uses ~/.cache/whisper by\n",
            "                        default (default: None)\n",
            "  --device DEVICE       device to use for PyTorch inference (default: cpu)\n",
            "  --output_dir OUTPUT_DIR, -o OUTPUT_DIR\n",
            "                        directory to save the outputs (default: .)\n",
            "  --output_format {txt,vtt,srt,tsv,json,all}, -f {txt,vtt,srt,tsv,json,all}\n",
            "                        format of the output file; if not specified, all\n",
            "                        available formats will be produced (default: all)\n",
            "  --verbose VERBOSE     whether to print out the progress and debug messages\n",
            "                        (default: True)\n",
            "  --task {transcribe,translate}\n",
            "                        whether to perform X->X speech recognition\n",
            "                        ('transcribe') or X->English translation ('translate')\n",
            "                        (default: transcribe)\n",
            "  --language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}\n",
            "                        language spoken in the audio, specify None to perform\n",
            "                        language detection (default: None)\n",
            "  --temperature TEMPERATURE\n",
            "                        temperature to use for sampling (default: 0)\n",
            "  --best_of BEST_OF     number of candidates when sampling with non-zero\n",
            "                        temperature (default: 5)\n",
            "  --beam_size BEAM_SIZE\n",
            "                        number of beams in beam search, only applicable when\n",
            "                        temperature is zero (default: 5)\n",
            "  --patience PATIENCE   optional patience value to use in beam decoding, as in\n",
            "                        https://arxiv.org/abs/2204.05424, the default (1.0) is\n",
            "                        equivalent to conventional beam search (default: None)\n",
            "  --length_penalty LENGTH_PENALTY\n",
            "                        optional token length penalty coefficient (alpha) as\n",
            "                        in https://arxiv.org/abs/1609.08144, uses simple\n",
            "                        length normalization by default (default: None)\n",
            "  --suppress_tokens SUPPRESS_TOKENS\n",
            "                        comma-separated list of token ids to suppress during\n",
            "                        sampling; '-1' will suppress most special characters\n",
            "                        except common punctuations (default: -1)\n",
            "  --initial_prompt INITIAL_PROMPT\n",
            "                        optional text to provide as a prompt for the first\n",
            "                        window. (default: None)\n",
            "  --carry_initial_prompt CARRY_INITIAL_PROMPT\n",
            "                        if True, prepend initial_prompt to every internal\n",
            "                        decode() call. May reduce the effectiveness of\n",
            "                        condition_on_previous_text (default: False)\n",
            "  --condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT\n",
            "                        if True, provide the previous output of the model as a\n",
            "                        prompt for the next window; disabling may make the\n",
            "                        text inconsistent across windows, but the model\n",
            "                        becomes less prone to getting stuck in a failure loop\n",
            "                        (default: True)\n",
            "  --fp16 FP16           whether to perform inference in fp16; True by default\n",
            "                        (default: True)\n",
            "  --temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK\n",
            "                        temperature to increase when falling back when the\n",
            "                        decoding fails to meet either of the thresholds below\n",
            "                        (default: 0.2)\n",
            "  --compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD\n",
            "                        if the gzip compression ratio is higher than this\n",
            "                        value, treat the decoding as failed (default: 2.4)\n",
            "  --logprob_threshold LOGPROB_THRESHOLD\n",
            "                        if the average log probability is lower than this\n",
            "                        value, treat the decoding as failed (default: -1.0)\n",
            "  --no_speech_threshold NO_SPEECH_THRESHOLD\n",
            "                        if the probability of the <|nospeech|> token is higher\n",
            "                        than this value AND the decoding has failed due to\n",
            "                        `logprob_threshold`, consider the segment as silence\n",
            "                        (default: 0.6)\n",
            "  --word_timestamps WORD_TIMESTAMPS\n",
            "                        (experimental) extract word-level timestamps and\n",
            "                        refine the results based on them (default: False)\n",
            "  --prepend_punctuations PREPEND_PUNCTUATIONS\n",
            "                        if word_timestamps is True, merge these punctuation\n",
            "                        symbols with the next word (default: \"'“¿([{-)\n",
            "  --append_punctuations APPEND_PUNCTUATIONS\n",
            "                        if word_timestamps is True, merge these punctuation\n",
            "                        symbols with the previous word (default:\n",
            "                        \"'.。,，!！?？:：”)]}、)\n",
            "  --highlight_words HIGHLIGHT_WORDS\n",
            "                        (requires --word_timestamps True) underline each word\n",
            "                        as it is spoken in srt and vtt (default: False)\n",
            "  --max_line_width MAX_LINE_WIDTH\n",
            "                        (requires --word_timestamps True) the maximum number\n",
            "                        of characters in a line before breaking the line\n",
            "                        (default: None)\n",
            "  --max_line_count MAX_LINE_COUNT\n",
            "                        (requires --word_timestamps True) the maximum number\n",
            "                        of lines in a segment (default: None)\n",
            "  --max_words_per_line MAX_WORDS_PER_LINE\n",
            "                        (requires --word_timestamps True, no effect with\n",
            "                        --max_line_width) the maximum number of words in a\n",
            "                        segment (default: None)\n",
            "  --threads THREADS     number of threads used by torch for CPU inference;\n",
            "                        supercedes MKL_NUM_THREADS/OMP_NUM_THREADS (default:\n",
            "                        0)\n",
            "  --clip_timestamps CLIP_TIMESTAMPS\n",
            "                        comma-separated list start,end,start,end,...\n",
            "                        timestamps (in seconds) of clips to process, where the\n",
            "                        last end timestamp defaults to the end of the file\n",
            "                        (default: 0)\n",
            "  --hallucination_silence_threshold HALLUCINATION_SILENCE_THRESHOLD\n",
            "                        (requires --word_timestamps True) skip silent periods\n",
            "                        longer than this threshold (in seconds) when a\n",
            "                        possible hallucination is detected (default: None)\n"
          ]
        }
      ],
      "source": [
        "!whisper --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhFGoWkGvobg"
      },
      "source": [
        "The above help command lists command-line arguments we can use. Most will be unnecessary for our case, while some may be more applicable such as `--output_format` or `--language`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwY47mQ0H0zq"
      },
      "source": [
        "## Transcribing Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCVul2DHvYkg"
      },
      "source": [
        "Let's now try and transcribe an audio file! The following command will transcribe speech in audio file `Audio File.mp3`. We will use the arguments\n",
        "\n",
        "1.   `--model medium`: specify the model size\n",
        "2.   `--task transcribe`: to do transcription\n",
        "3.   `--output_dir transcription`: to save the files in the directory transcription\n",
        "4.   `--output_format all`: give transcription in all formats. Otherwise, you may select the one your prefer.\n",
        "\n",
        "Some caveats\n",
        "\n",
        "\n",
        "*   If you are running this without a GPU, you should include: `--device cpu`\n",
        "*   If you are interested in <ins>translating</ins> an audio file, you can use a command such as `!whisper japanese_audio_file.wav --language Japanese --task translate --model medium --output_dir translation --output_format all`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72kipZ36H0zq"
      },
      "source": [
        "We must specify the location of the audio file. In this case it's in `/content/AllStar.mp3`. We can determine this by rightclicking the audiofile and Copying the Path."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper /content/AllStar.mp3 --model medium --device gpu --language hu --task transcribe --output_dir transcription --output_format all"
      ],
      "metadata": {
        "id": "htH18PY_erF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj0xZ9EpH0zr"
      },
      "source": [
        "**Note**:\n",
        "- Most of the time you won't want to use the `all` output format as it will generate some unnecessary files. Its best to determine which format works for you. Recall, you can find the available formats by running `whisper --help`.\n",
        "- If you are running this on your own computer, the path for the file will be different. You can find the path by right-clicking the file and selecting \"Copy Path\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h1iid5LH0zr"
      },
      "source": [
        "## Translating Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7sRA2QpH0zr"
      },
      "source": [
        "Let's now try and translate an audio file! Make sure you uploaded the `Cupid_Fifty_Fifty_Korean_Version.mp3` file to Colab. To see which languages Whisper supports, you can run `!whisper --help` and look at the `--language` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQlvBQg6H0zr"
      },
      "outputs": [],
      "source": [
        "!whisper --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dmurhZdH0zt"
      },
      "source": [
        "Suppose we have a Korean audio file and we want to translate it to English. We can use the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvBaB6hSH0zt"
      },
      "outputs": [],
      "source": [
        "!whisper /content/Cupid_Fifty_Fifty_Korean_Version.mp3 --language Korean --task translate --model medium --output_dir translation --output_format all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am205LnZH0zt"
      },
      "source": [
        "## Creating a Caption File (SRT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoFN644oH0zt"
      },
      "source": [
        "Suppose we want to take an audio file and create a caption file (SRT) for it. SRT files are used to display subtitles in videos and are commonly used in video editing software or on YouTube. Let's try creating an SRT file for an audio file. We will use the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEMdKgdYH0zt"
      },
      "outputs": [],
      "source": [
        "!whisper /content/AllStar.mp3 --model medium --task transcribe --output_dir transcription --output_format srt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxOEn-GOH0zu"
      },
      "source": [
        "What if the audio is in a different language? We can use the `--language` argument to specify the language of the audio file. For example, if the audio file is in Korean, we can use the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp5gEiEfH0zu"
      },
      "outputs": [],
      "source": [
        "!whisper /content/Cupid_Fifty_Fifty_Korean_Version.mp3 --language Korean --task translate --model medium --output_dir translation --output_format srt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJYNy7mCH0zu"
      },
      "source": [
        "## Saving Your Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqTQIgOPyEmk"
      },
      "source": [
        "Once you've ran all the cells you're interested in, we should download the transcription files that are saved to the `transcription` directory.\n",
        "\n",
        "Colab doesn't allow you to download entire directories so we can instead `zip` it and download the zip file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFBt8cHZyRKq"
      },
      "outputs": [],
      "source": [
        "!zip -r transcriptions.zip transcription\n",
        "!zip -r translations.zip translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmRHI2u0yf-O"
      },
      "source": [
        "## Helpful Commands\n",
        "\n",
        "\n",
        "\n",
        "*   Often, you fill note want the transcription in every format. It may instead be better to select the most usable one for your case and just use that as the `--output_format`\n",
        "*   You can do multiple audio files at a time. You would do the same command, but include two or more audio files (e.g. `!whisper audio1.mp3 audio2.mp3 ...`)\n",
        "* If you are running whisper without a GPU, you should include `--device cpu` in the command\n",
        "* If you want each caption segment to be 3 words max opposed to sentences you could do something like\n",
        "```bash\n",
        "  whisper audio_file.mp3 --task transcribe --output_format srt --word_timestamps True --max_words_per_line 3 # for transcription`\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD3RbYIgzAR5"
      },
      "source": [
        "## Closing Remarks\n",
        "\n",
        "Using Whisper in Colab is convenient because it allows you to utilize a GPU for free! However, you can also use Whisper without a GPU on your local computer. To do this, you should follow the steps in Whisper's [Github Setup](https://github.com/openai/whisper#setup). If you choose to do so, some considerations\n",
        "\n",
        "\n",
        "\n",
        "1.   You will need to install Python to run whisper. This can be done in your terminal by first installing [brew](https://docs.brew.sh/Installation) with the command `/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"`. We recommend following `HomeBrew's [installation instructions.](https://brew.sh/) or this [guide](https://docs.python-guide.org/starting/install3/osx/).\n",
        "2.   Assuming brew is installed, you can install python with running `brew install python` inside your terminal.\n",
        "3.   With Python and brew installed, we recommend making a directory to work in. Inside your terminal, move to your desktop and create a directory: `cd Desktop; mkdir Whisper; cd Whisper`.\n",
        "\n",
        "Note: if you do wish to work on your personal macbook and do install brew, you will need to also install `Xcode` tools. If given the option, we recommend instead installing `Command Line Tools` as it is a much smaller package.\n",
        "\n",
        "Finally, while this document is primarily for mac users, the installation will be very similar across platforms. See Whisper's Github for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2X3ZBlF3xkr"
      },
      "source": [
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "</head>\n",
        "<body>\n",
        "     <img src=\"https://media.istockphoto.com/id/1294688589/photo/red-cat-with-blurred-the-poster-in-the-frame-with-the-words-thank-you.jpg?s=612x612&w=0&k=20&c=T84nHSu52sOQvrmnksdDNo2UByqJ7yXn1srkuodXdps=\" alt=\"Page Image\">\n",
        "    <br>\n",
        "</body>\n",
        "</html>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.10"
      },
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}